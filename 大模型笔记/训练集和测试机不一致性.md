
### 核心背景
当模型在测试集上表现远好于训练集，且排除模型配置问题后，需排查**训练集和测试集的不一致性**，并通过特定方法检测与缓解。

### 一、先排查技术问题
在分析数据集前，先检查数据加载和评测代码的问题：
- 做健全性检查：用训练集临时替换测试集重新评测，若两者表现仍有差异，大概率是代码存在bug（常与数据重排错误、数据归一化不一致/缺失有关）。

### 二、数据集分布差异的检测
若排除代码问题，测试集表现优于训练集则可排除过拟合，更可能是**训练/测试数据分布显著差异**（影响特征或目标变量），检测方法包括：
1. **可视化分布**：绘制训练/测试数据的目标/标签分布图；小型表格数据集可通过直方图对比特征分布。
2. **对抗验证**：
    - 核心：识别训练与测试数据的相似程度，是图像、文本数据的通用检测方法。
    - 流程（图29-1）：合并训练集与测试集→创建二元目标变量（标记“是否测试数据”）→用k折交叉验证或重新划分数据集，训练模型预测该二元标签。
    - 结果解读：模型预测“是否测试数据”表现好→训练/测试数据分布差异大；表现差→分布相似。

### 三、分布差异的缓解策略
若对抗验证检测出差异，可采取这些方法：
1. **表格数据集**：逐一移除特征，观察是否缓解问题；用带更新目标的序列特征选择算法，最小化分类准确度（而非最大化）。
2. **图像/文本文集**：研究移除与测试集不同的个别训练实例，尝试解决分布差异。

### 四、练习问题
1. 对抗预测任务中，良好的表现基准是什么？
2. 训练集通常比测试集大，对抗验证会因标签不平衡（多数示例标记为“否”）导致预测问题，这是否是问题？若是，该如何缓解？
