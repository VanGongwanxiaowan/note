
### 核心背景
人类评测生成式大模型虽准确但耗时、主观且难以自动化，因此采用**困惑度、BLEU、ROUGE、BERTScore**等客观指标量化模型性能；这些指标分为**内在指标**（与模型训练损失直接相关）和**外在指标**（与任务效果直接相关）。

### 一、大模型的评测指标分类
- **内在指标**：以**困惑度**为代表，与训练过程的损失函数（交叉熵）直接相关。
- **外在指标**：以**BLEU、ROUGE**为代表，与任务的精度、召回率相关（BLEU对应精度，ROUGE对应召回率）。

### 二、核心评测指标详解
1. **困惑度（19.1.1）**
    - **定义**：内在指标，量化模型在给定上下文中预测下一个词的平均不确定性，与交叉熵密切相关，公式为$Perplexity(s) = 2^{-\frac{1}{n}\log_2(p(s))}$（$s$为评测文本，$n$为词/词元数量，$p(s)$为模型预测概率）。
    - **解读**：困惑度越低，模型表现越好；示例中“迅捷的棕色狐狸跳过懒惰的狗”困惑度约1.043，“迅捷的黑猫跳过懒惰的狗”困惑度约2.419，后者模型预测不确定性更高。

2. **BLEU（19.1.2）**
    - **定义**：基于精度的外在指标，用于评测机器翻译质量，衡量生成译文与人工参考译文的$n$-gram重叠程度（实践中常用4-gram），得分范围0（最差）-1（最好）。
    - **计算步骤**（图19-1，以1-gram为例）：统计候选文本在参考文本中的词数→计算精度→加权精度（裁剪重复词数）→乘以短句惩罚因子（避免短译文得分偏高）。
    - **缺点**：仅衡量字符串相似度，无法识别同义词/语序变化，可能给语义准确但表述不同的翻译打低分；与人类评测的相关性后续被证伪。
    - **价值**：可作为模型训练改进的评估工具，不适合作为最终评测工具，替代方案有METEOR、COMET。

3. **ROUGE（19.1.3）**
    - **定义**：基于召回率（现代为F1分数）的外在指标，主要用于评测自动摘要，也可用于机器翻译，衡量参考文本的词在生成摘要中的出现比例。
    - **计算步骤**（图19-2，以1-gram为例）：计算召回率→计算精度→计算F1分数（调和平均值）。
    - **变体**：ROUGE-N（$n$-gram重叠）、ROUGE-L（最长公共子序列）、ROUGE-S（跳跃二元组重叠）。
    - **缺点**：与BLEU类似，未考虑同义词/意译，依赖$n$-gram重叠；但2021年计算语言学会议中69%的摘要模型论文仍用其评估。

4. **BERTScore（19.1.4）**
    - **定义**：外在指标，利用预训练BERT模型的上下文嵌入，通过余弦相似度衡量候选文本与参考文本的语义相似性，适用于翻译和摘要评测。
    - **计算步骤**（图19-3）：生成候选/参考文本→分词→BERT生成词嵌入→计算嵌入的余弦相似度→匹配最高相似度词元→计算平均相似度得最终分数。
    - **优势**：比BLEU、ROUGE更能捕捉语义相似性，对释义更稳健；**缺点**：计算成本更高，需依赖预训练BERT模型。

### 三、替代指标与补充说明（19.2）
- 所有指标均为“替代指标”，人工评测是最佳方式但成本高、难规模化；NLP数据集往往无法反映模型实际使用场景，指标仅能估算性能。
- 除上述指标外，还有其他流行指标用于评测大模型预测性能。

### 四、练习问题（19.3）
1. 图19-3中“cat”的两个嵌入余弦相似度非1.0的原因（1.0为最大相似度）。
2. BERTScore不具备对称性（交换候选/参考文本位置得分不同）的解决方法。

### 五、参考文献（19.4）
列出BLEU相关的核心研究论文，包括原始方法提出、相关性证伪、缺点总结等文献。

我可以帮你整理**各评测指标的适用场景与优缺点对照表**，把困惑度、BLEU、ROUGE、BERTScore的核心特点和适用任务汇总，方便你快速选择指标，需要吗？
